# -*- coding: utf-8 -*-
"""[Pelatihan Model] Submission Proyek Analisis Sentimen Review Aplikasi Webtoon_MC006D5X1393.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jCXXImsDE95O_QJDOGMsNtXQvLSMWTfe

## **Tugas Proyek Analisis Setimen**
Nama : Nisrina Fatimah Parisya

ID Cohort : MC006D5X1393

Tema Tugas : Analisis Sentimen pada Aplikasi
"""

!pip install sastrawi
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

!pip install swifter

"""# Import Library"""

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import nltk
import csv
import string
from tqdm.notebook import tqdm
tqdm.pandas()
import time
import swifter
swifter.set_defaults(display_progressbar=True)
import requests
from io import StringIO
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.linear_model import SGDClassifier

nltk.download('punkt_tab', force=True)
nltk.download('stopwords', force=True)
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

"""# Load Dataset"""

df_webtoon_review = pd.read_csv('/content/webtun/review_webtoon.csv')

df_webtoon_review.info()

def clean_webtoon_data(df):
    # 1. Hapus baris yang tidak punya isi review
    df = df[df['content'].notna()].copy()

    # 2. Isi nilai kosong di kolom yang tidak krusial
    df['userImage'] = df['userImage'].fillna('default.jpg')
    df['reviewCreatedVersion'] = df['reviewCreatedVersion'].fillna('unknown')
    df['appVersion'] = df['appVersion'].fillna('unknown')
    df['replyContent'] = df['replyContent'].fillna('no reply')

    # 3. Isi nilai kosong di kolom tanggal dengan pd.NaT (null datetime)
    df['at'] = df['at'].fillna(pd.NaT)
    df['repliedAt'] = df['repliedAt'].fillna(pd.NaT)

    # 4. Isi angka kosong jika ada (optional, tergantung kebutuhan)
    if df['score'].isnull().any():
        df['score'] = df['score'].fillna(df['score'].median())
    if df['thumbsUpCount'].isnull().any():
        df['thumbsUpCount'] = df['thumbsUpCount'].fillna(0)

    return df

df_cleaned = clean_webtoon_data(df_webtoon_review)
print(df_cleaned.info())

"""# Preprocesing Text"""

def cleaningText(text):
    text = text.translate(str.maketrans('', '', string.punctuation))
    text = text.strip(' ')
    text = text.replace('\n', ' ').strip()
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'#[^a-zA-Z\s]', ' ', text)
    text = re.sub(r'@[^a-zA-Z\s]', ' ', text)
    text = re.sub(r'http\S+', ' ', text)
    text = re.sub(r'\d+', ' ', text)
    return text

def casefoldingText(text):
    text = text.lower()
    return text


def tokenizing(text):
    return word_tokenize(text)

def filterText(tokens):
    stop_words = set(stopwords.words('indonesian') + stopwords.words('english'))
    stop_words.update([
        'yea','yaa','di','wei','woy','deh','dong','tuh','gitu','aja',
        'kah','lah','gak','loh','duh','dih','ku','gaa','yaaa','ntar','lg'
    ])
    return [word for word in tokens if word not in stop_words]

def stemmingText(text):
    factory = StemmerFactory()
    stemmer = factory.create_stemmer()
    stemmed_text = stemmer.stem(' '.join(text))
    return stemmed_text.split()

def ubahKalimat(daftarkalimat):
    return ' '.join(daftarkalimat)

slang = {
    "lol": "lucu",
    "abis": "habis",
    "bgt": "sangat",
    "btw": "ngomong-ngomong",
    "fix": "pasti",
    "delay": "lambat merespons",
    "ngaco": "kacau",
    "app" : "aplikasi",
    "crash": "aplikasi berhenti",
    "lemot": "lama",
    "instal":"mengunduh",
    "install":"mengunduh",
    "ngulang": "mengulang",
    "ribet": "rumit",
    "guys": "teman-teman",
    "kayak": "seperti",
    "apk": "aplikasi",
    "kepo": "ingin tahu",
    "error": "gagal",
    "bug": "masalah teknis",
    "glitch": "gangguan tampilan",
    "loading": "memuat",
    "buffering": "proses loading lama",
    "skip": "lewati",
    "spoiler": "bocoran cerita",
    "update": "pembaruan",
    "komen": "komentar",
    "rate": "penilaian",
    "fav": "favorit",
    "rekomend": "rekomendasi",
    "kocak": "lucu",
    "gemoy": "imut",
    "ship": "dukung pasangan",
    "plot": "alur cerita",
    "cliff": "cliffhanger",
    "teaser": "cuplikan",
    "spoiler": "bocoran",
    "scroll": "gulir",
    "zoom": "perbesar",
    "resolusi": "kualitas gambar",
    "darkmode": "mode gelap",
    "notif": "notifikasi",
    "subs": "berlangganan",
    "premium": "berbayar",
    "free": "gratis",
    "ads": "iklan",
    "lag": "lambat",
    "stuck": "macet",
    "kudu": "harus",
    "kuy": "ayo",
    "cmiiw": "koreksi jika saya salah",
    "gercep": "cepat tanggap",
    "hiks": "sedih",
    "wkwk": "tertawa",
    "mager": "malas gerak",
    "santuy": "santai",
    "panutan": "contoh baik",
    "nyesel": "menyesal",
    "bet": "nyaman",
    "jadul": "jaman dulu",
    "jleb": "kena hati",
    "salfok": "salah fokus",
    "sabi": "salah sendiri",
    "gaje": "gak jelas",
    "bucin": "budak cinta",
    "baper": "bawa perasaan",
    "ygy": "ya guys ya",
    "sist": "sister (kakak/adik)",
    "bro": "brother (teman)",
    "anjay": "wah (ekspresi)",
    "sokin": "sok penting",
    "gws": "semoga cepat sembuh",
    "receh": "lucu receh",
    "sotoy": "sok tahu",
    "typo": "salah ketik",
    "yha": "ya sudah",

}
def example_slangwords(text):
    words = text.split()
    katafix = []
    for kata in words:
        if kata.lower() in slang:
            katafix.append(slang[kata.lower()])
        else:
            katafix.append(kata)
    return ' '.join(katafix)

#df baru biar lebih aman buat modeling data nanti
df_processed = df_cleaned.copy()

df_processed['clean_text'] = df_processed['content'].progress_apply(cleaningText)
df_processed ['text_casefoldingText'] = df_processed['clean_text'].progress_apply(casefoldingText)
df_processed['slang_fixed'] = df_processed['text_casefoldingText'].progress_apply(example_slangwords)
df_processed['tokens'] = df_processed['slang_fixed'].progress_apply(tokenizing)
df_processed['text_stopword'] = df_processed['tokens'].progress_apply(filterText)
df_processed['final_text'] = df_processed['text_stopword'].progress_apply(ubahKalimat)
df_processed.head()

"""# Labeling Data"""

def load_lexicon(url):
    lexicon = {}
    response = requests.get(url)

    if response.status_code == 200:
        reader = csv.reader(StringIO(response.text), delimiter=',')
        for row in reader:
            lexicon[row[0]] = int(row[1])
    else:
        print(f"Gagal mengambil data dari: {url}")

    return lexicon

# URL untuk kamus kata positif dan negatif
url_positive = 'https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv'
url_negative = 'https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv'

# Panggil fungsi untuk ambil datanya
lexicon_positive = load_lexicon(url_positive)
lexicon_negative = load_lexicon(url_negative)

def hitung_sentimen(kata_token):
    skor = 0
    positif_ditemukan = []
    negatif_ditemukan = []

    for kata in kata_token:
        if kata in lexicon_positive:
            skor += lexicon_positive[kata]
            positif_ditemukan.append(kata)
        elif kata in lexicon_negative:
            skor += lexicon_negative[kata]
            negatif_ditemukan.append(kata)

    if skor > 0:
        polaritas = 'positif'
    elif skor < 0:
        polaritas = 'negatif'
    else:
        polaritas = 'netral'

    return skor, polaritas

results = df_processed['filtered_tokens'].apply(hitung_sentimen)

# Pisahkan hasil tuple jadi dua kolom
results = list(zip(*results))
df_processed['skor_polaritas'] = results[0]
df_processed['polaritas'] = results[1]

# Cek distribusi polaritas
print(df_processed['polaritas'].value_counts())

polarity_counts = df_processed['polaritas'].value_counts()

# visualisasi
colors =['#FFB6C1', '#FF69B4', '#DDA0DD']
plt.figure(figsize=(8, 8))
plt.pie(polarity_counts, labels=polarity_counts.index, autopct='%1.1f%%', startangle=90, colors=colors)
plt.title('Distribusi Polaritas Sentimen Review Webtoon', fontsize=14)
plt.axis('equal')

plt.show()

"""# Data Spiltting"""

# Split data
X = df_processed['final_text']
y = df_processed['polaritas']

# melakukan proses TF-IDF
vectorizer = TfidfVectorizer(max_features=10000)
X_tfidf = vectorizer.fit_transform(X)

"""#Modeling Data

#### Random Forest (80/20)
"""

# Split data (80/20)
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# Check bentuk training dan testing dataset
print(f"Training set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")
print(f"Memory usage is now optimized by using sparse matrices")

# inisialisasi
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42) # You can adjust n_estimators

# latih data
rf_classifier.fit(X_train, y_train)

# buat prediksi
y_pred_rf = rf_classifier.predict(X_test)

# evaluasi
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {accuracy_rf}")
print(classification_report(y_test, y_pred_rf))

"""#### Logistic Regresion (70/30)"""

# Split data (70/30)
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)

# Check shapes of training and testing datasets
print(f"Training set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")
print(f"Memory usage is now optimized by using sparse matrices")

# Inisialisasi model Logistic Regression
logreg_classifier = LogisticRegression(max_iter=1000, random_state=42) # Menambah max_iter

# training
logreg_classifier.fit(X_train, y_train)

# Buat prediksi
y_pred_logreg = logreg_classifier.predict(X_test)

# Evaluasi
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)
print(f"Logistic Regression Accuracy: {accuracy_logreg}")
print(classification_report(y_test, y_pred_logreg))

"""#### SGDClassifier (70/30)"""

# Split data (70/30)
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)

# Check shapes of training and testing datasets
print(f"Training set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")
print(f"Memory usage is now optimized by using sparse matrices")

# Inisialisasi model SGDClassifier
sgd_classifier = SGDClassifier(random_state=42)

# Latih model
sgd_classifier.fit(X_train, y_train)

# Buat prediksi
y_pred_sgd = sgd_classifier.predict(X_test)

# Evaluasi model
accuracy_sgd = accuracy_score(y_test, y_pred_sgd)
print(f"SGD Classifier Accuracy: {accuracy_sgd}")
print(classification_report(y_test, y_pred_sgd))

"""# Proses Inference"""

def predict_sentiment(text, model=logreg_classifier, vectorizer=vectorizer):
    # Preprocess the input text
    cleaned_text = cleaningText(text)
    casefolded_text = casefoldingText(cleaned_text)
    slang_fixed_text = example_slangwords(casefolded_text)
    tokens = tokenizing(slang_fixed_text)
    filtered_tokens = filterText(tokens)
    final_text = ubahKalimat(filtered_tokens)

    # Vectorize the text using the same vectorizer used during training
    text_tfidf = vectorizer.transform([final_text])

    # Predict the sentiment using the chosen model
    prediction = model.predict(text_tfidf)[0]

    return prediction

# Example usage
user_input = input("Enter a review: ")
predicted_sentiment = predict_sentiment(user_input)
print(f"Predicted sentiment: {predicted_sentiment}")

"""# Requirenment"""

!pip install pipreqs

!pip freeze | grep 'pandas\|numpy\|matplotlib\|seaborn\|nltk\|tqdm\|swifter\|requests\|wordcloud' > requirements.txt
!cat requirements.txt